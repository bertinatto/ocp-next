From ba232710f73f270975a6b83706701b861dd4e002 Mon Sep 17 00:00:00 2001
From: Maciej Szulik <maszulik@redhat.com>
Date: Wed, 22 Feb 2023 13:43:58 +0100
Subject: [PATCH] UPSTREAM: <carry>: Create minimal wrapper needed to run k8s
 e2e tests

UPSTREAM: <carry>: Change annotation mechanics to allow injecting testMaps and filter out tests

UPSTREAM: <carry>: Move k8s-specific rules to our fork

UPSTREAM: <carry>: Create minimal wrapper needed to run k8s e2e tests
---
 hack/lib/golang.sh                            |   3 +-
 openshift-hack/cmd/k8s-tests/k8s-tests.go     |  98 +++++++++++
 openshift-hack/cmd/k8s-tests/provider.go      | 147 ++++++++++++++++
 openshift-hack/cmd/k8s-tests/runtest.go       | 140 +++++++++++++++
 openshift-hack/cmd/k8s-tests/types.go         |  69 ++++++++
 openshift-hack/e2e/annotate/annotate.go       |  17 +-
 openshift-hack/e2e/annotate/cmd/main.go       |   2 +-
 openshift-hack/e2e/annotate/rules.go          | 130 +++++++++++++-
 openshift-hack/e2e/annotate/rules_test.go     |   2 +-
 openshift-hack/e2e/namespace.go               | 160 ++++++++++++++++++
 .../images/hyperkube/Dockerfile.rhel          |   4 +-
 11 files changed, 760 insertions(+), 12 deletions(-)
 create mode 100644 openshift-hack/cmd/k8s-tests/k8s-tests.go
 create mode 100644 openshift-hack/cmd/k8s-tests/provider.go
 create mode 100644 openshift-hack/cmd/k8s-tests/runtest.go
 create mode 100644 openshift-hack/cmd/k8s-tests/types.go
 create mode 100644 openshift-hack/e2e/namespace.go

diff --git a/hack/lib/golang.sh b/hack/lib/golang.sh
index b78be169c06..c8cdeef545e 100755
--- a/hack/lib/golang.sh
+++ b/hack/lib/golang.sh
@@ -81,6 +81,7 @@ kube::golang::server_targets() {
     vendor/k8s.io/apiextensions-apiserver
     cluster/gce/gci/mounter
     cmd/watch-termination
+    openshift-hack/cmd/k8s-tests
   )
   echo "${targets[@]}"
 }
@@ -440,7 +441,7 @@ kube::golang::set_platform_envs() {
 
   # if CC is defined for platform then always enable it
   ccenv=$(echo "$platform" | awk -F/ '{print "KUBE_" toupper($1) "_" toupper($2) "_CC"}')
-  if [ -n "${!ccenv-}" ]; then 
+  if [ -n "${!ccenv-}" ]; then
     export CGO_ENABLED=1
     export CC="${!ccenv}"
   fi
diff --git a/openshift-hack/cmd/k8s-tests/k8s-tests.go b/openshift-hack/cmd/k8s-tests/k8s-tests.go
new file mode 100644
index 00000000000..fedd8b16f01
--- /dev/null
+++ b/openshift-hack/cmd/k8s-tests/k8s-tests.go
@@ -0,0 +1,98 @@
+package main
+
+import (
+	"encoding/json"
+	"flag"
+	"fmt"
+	"math/rand"
+	"os"
+	"sort"
+	"time"
+
+	"github.com/spf13/cobra"
+	"github.com/spf13/pflag"
+
+	utilflag "k8s.io/component-base/cli/flag"
+	"k8s.io/component-base/logs"
+	"k8s.io/kubernetes/test/e2e/framework"
+
+	// initialize framework extensions
+	_ "k8s.io/kubernetes/test/e2e/framework/debug/init"
+	_ "k8s.io/kubernetes/test/e2e/framework/metrics/init"
+)
+
+func main() {
+	logs.InitLogs()
+	defer logs.FlushLogs()
+
+	rand.Seed(time.Now().UTC().UnixNano())
+
+	pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc)
+
+	root := &cobra.Command{
+		Long: "OpenShift Tests compatible wrapper",
+	}
+
+	root.AddCommand(
+		newRunTestCommand(),
+		newListTestsCommand(),
+	)
+
+	f := flag.CommandLine.Lookup("v")
+	root.PersistentFlags().AddGoFlag(f)
+	pflag.CommandLine = pflag.NewFlagSet("empty", pflag.ExitOnError)
+	flag.CommandLine = flag.NewFlagSet("empty", flag.ExitOnError)
+	framework.RegisterCommonFlags(flag.CommandLine)
+	framework.RegisterClusterFlags(flag.CommandLine)
+
+	if err := func() error {
+		return root.Execute()
+	}(); err != nil {
+		if ex, ok := err.(ExitError); ok {
+			fmt.Fprintf(os.Stderr, "Ginkgo exit error %d: %v\n", ex.Code, err)
+			os.Exit(ex.Code)
+		}
+		fmt.Fprintf(os.Stderr, "error: %v\n", err)
+		os.Exit(1)
+	}
+}
+
+func newRunTestCommand() *cobra.Command {
+	testOpt := NewTestOptions(os.Stdout, os.Stderr)
+
+	cmd := &cobra.Command{
+		Use:          "run-test NAME",
+		Short:        "Run a single test by name",
+		Long:         "Execute a single test.",
+		SilenceUsage: true,
+		RunE: func(cmd *cobra.Command, args []string) error {
+			if err := initializeTestFramework(os.Getenv("TEST_PROVIDER")); err != nil {
+				return err
+			}
+
+			return testOpt.Run(args)
+		},
+	}
+	return cmd
+}
+
+func newListTestsCommand() *cobra.Command {
+	cmd := &cobra.Command{
+		Use:          "list",
+		Short:        "List available tests",
+		Long:         "List the available tests in this binary.",
+		SilenceUsage: true,
+		RunE: func(cmd *cobra.Command, args []string) error {
+			tests := testsForSuite()
+			sort.Slice(tests, func(i, j int) bool { return tests[i].Name < tests[j].Name })
+			data, err := json.Marshal(tests)
+			if err != nil {
+				return err
+			}
+			fmt.Fprintf(os.Stdout, "%s\n", data)
+			return nil
+		},
+	}
+
+	return cmd
+}
diff --git a/openshift-hack/cmd/k8s-tests/provider.go b/openshift-hack/cmd/k8s-tests/provider.go
new file mode 100644
index 00000000000..48ee23fce76
--- /dev/null
+++ b/openshift-hack/cmd/k8s-tests/provider.go
@@ -0,0 +1,147 @@
+package main
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+	"os"
+	"path/filepath"
+	"strings"
+
+	"github.com/onsi/ginkgo/v2"
+	"github.com/onsi/gomega"
+
+	corev1 "k8s.io/api/core/v1"
+	kclientset "k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/tools/clientcmd"
+	"k8s.io/kubernetes/openshift-hack/e2e"
+	conformancetestdata "k8s.io/kubernetes/test/conformance/testdata"
+	"k8s.io/kubernetes/test/e2e/framework"
+	"k8s.io/kubernetes/test/e2e/framework/testfiles"
+	"k8s.io/kubernetes/test/e2e/storage/external"
+	e2etestingmanifests "k8s.io/kubernetes/test/e2e/testing-manifests"
+	testfixtures "k8s.io/kubernetes/test/fixtures"
+
+	// this appears to inexplicably auto-register global flags.
+	_ "k8s.io/kubernetes/test/e2e/storage/drivers"
+
+	// these are loading important global flags that we need to get and set
+	_ "k8s.io/kubernetes/test/e2e"
+	_ "k8s.io/kubernetes/test/e2e/lifecycle"
+)
+
+// copied directly from github.com/openshift/origin/cmd/openshift-tests/provider.go
+// and github.com/openshift/origin/test/extended/util/test.go
+func initializeTestFramework(provider string) error {
+	providerInfo := &ClusterConfiguration{}
+	if err := json.Unmarshal([]byte(provider), &providerInfo); err != nil {
+		return fmt.Errorf("provider must be a JSON object with the 'type' key at a minimum: %v", err)
+	}
+	if len(providerInfo.ProviderName) == 0 {
+		return fmt.Errorf("provider must be a JSON object with the 'type' key")
+	}
+	config := &ClusterConfiguration{}
+	if err := json.Unmarshal([]byte(provider), config); err != nil {
+		return fmt.Errorf("provider must decode into the ClusterConfig object: %v", err)
+	}
+
+	// update testContext with loaded config
+	testContext := &framework.TestContext
+	testContext.Provider = config.ProviderName
+	testContext.CloudConfig = framework.CloudConfig{
+		ProjectID:   config.ProjectID,
+		Region:      config.Region,
+		Zone:        config.Zone,
+		Zones:       config.Zones,
+		NumNodes:    config.NumNodes,
+		MultiMaster: config.MultiMaster,
+		MultiZone:   config.MultiZone,
+		ConfigFile:  config.ConfigFile,
+	}
+	testContext.AllowedNotReadyNodes = -1
+	testContext.MinStartupPods = -1
+	testContext.MaxNodesToGather = 0
+	testContext.KubeConfig = os.Getenv("KUBECONFIG")
+
+	// allow the CSI tests to access test data, but only briefly
+	// TODO: ideally CSI would not use any of these test methods
+	// var err error
+	// exutil.WithCleanup(func() { err = initCSITests(dryRun) })
+	// TODO: for now I'm only initializing CSI directly, but we probably need that
+	// WithCleanup here as well
+	if err := initCSITests(); err != nil {
+		return err
+	}
+
+	if ad := os.Getenv("ARTIFACT_DIR"); len(strings.TrimSpace(ad)) == 0 {
+		os.Setenv("ARTIFACT_DIR", filepath.Join(os.TempDir(), "artifacts"))
+	}
+
+	testContext.DeleteNamespace = os.Getenv("DELETE_NAMESPACE") != "false"
+	testContext.VerifyServiceAccount = true
+	testfiles.AddFileSource(e2etestingmanifests.GetE2ETestingManifestsFS())
+	testfiles.AddFileSource(testfixtures.GetTestFixturesFS())
+	testfiles.AddFileSource(conformancetestdata.GetConformanceTestdataFS())
+	testContext.KubectlPath = "kubectl"
+	// context.KubeConfig = KubeConfigPath()
+	testContext.KubeConfig = os.Getenv("KUBECONFIG")
+
+	// "debian" is used when not set. At least GlusterFS tests need "custom".
+	// (There is no option for "rhel" or "centos".)
+	testContext.NodeOSDistro = "custom"
+	testContext.MasterOSDistro = "custom"
+
+	// load and set the host variable for kubectl
+	clientConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(&clientcmd.ClientConfigLoadingRules{ExplicitPath: testContext.KubeConfig}, &clientcmd.ConfigOverrides{})
+	cfg, err := clientConfig.ClientConfig()
+	if err != nil {
+		return err
+	}
+	testContext.Host = cfg.Host
+
+	// Ensure that Kube tests run privileged (like they do upstream)
+	testContext.CreateTestingNS = func(ctx context.Context, baseName string, c kclientset.Interface, labels map[string]string) (*corev1.Namespace, error) {
+		return e2e.CreateTestingNS(baseName, c, labels, true)
+	}
+
+	gomega.RegisterFailHandler(ginkgo.Fail)
+
+	framework.AfterReadingAllFlags(testContext)
+	testContext.DumpLogsOnFailure = true
+
+	// these constants are taken from kube e2e and used by tests
+	testContext.IPFamily = "ipv4"
+	if config.HasIPv6 && !config.HasIPv4 {
+		testContext.IPFamily = "ipv6"
+	}
+
+	testContext.ReportDir = os.Getenv("TEST_JUNIT_DIR")
+
+	return nil
+}
+
+const (
+	manifestEnvVar = "TEST_CSI_DRIVER_FILES"
+)
+
+// copied directly from github.com/openshift/origin/cmd/openshift-tests/csi.go
+// Initialize openshift/csi suite, i.e. define CSI tests from TEST_CSI_DRIVER_FILES.
+func initCSITests() error {
+	manifestList := os.Getenv(manifestEnvVar)
+	if manifestList != "" {
+		manifests := strings.Split(manifestList, ",")
+		for _, manifest := range manifests {
+			if err := external.AddDriverDefinition(manifest); err != nil {
+				return fmt.Errorf("failed to load manifest from %q: %s", manifest, err)
+			}
+			// Register the base dir of the manifest file as a file source.
+			// With this we can reference the CSI driver's storageClass
+			// in the manifest file (FromFile field).
+			testfiles.AddFileSource(testfiles.RootFileSource{
+				Root: filepath.Dir(manifest),
+			})
+		}
+	}
+
+	return nil
+}
diff --git a/openshift-hack/cmd/k8s-tests/runtest.go b/openshift-hack/cmd/k8s-tests/runtest.go
new file mode 100644
index 00000000000..35a6f2a14d0
--- /dev/null
+++ b/openshift-hack/cmd/k8s-tests/runtest.go
@@ -0,0 +1,140 @@
+package main
+
+import (
+	"fmt"
+	"io"
+	"regexp"
+	"strings"
+	"time"
+
+	"github.com/onsi/ginkgo/v2"
+	"github.com/onsi/ginkgo/v2/types"
+
+	"k8s.io/kubernetes/openshift-hack/e2e/annotate/generated"
+
+	// ensure all the ginkgo tests are loaded
+	_ "k8s.io/kubernetes/openshift-hack/e2e"
+)
+
+// TestOptions handles running a single test.
+type TestOptions struct {
+	Out    io.Writer
+	ErrOut io.Writer
+}
+
+var _ ginkgo.GinkgoTestingT = &TestOptions{}
+
+func NewTestOptions(out io.Writer, errOut io.Writer) *TestOptions {
+	return &TestOptions{
+		Out:    out,
+		ErrOut: errOut,
+	}
+}
+
+func (opt *TestOptions) Run(args []string) error {
+	if len(args) != 1 {
+		return fmt.Errorf("only a single test name may be passed")
+	}
+
+	// Ignore the upstream suite behavior within test execution
+	ginkgo.GetSuite().ClearBeforeAndAfterSuiteNodes()
+	tests := testsForSuite()
+	var test *TestCase
+	for _, t := range tests {
+		if t.Name == args[0] {
+			test = t
+			break
+		}
+	}
+	if test == nil {
+		return fmt.Errorf("no test exists with that name: %s", args[0])
+	}
+
+	suiteConfig, reporterConfig := ginkgo.GinkgoConfiguration()
+	suiteConfig.FocusStrings = []string{fmt.Sprintf("^ %s$", regexp.QuoteMeta(test.Name))}
+
+	// These settings are matched to upstream's ginkgo configuration. See:
+	// https://github.com/kubernetes/kubernetes/blob/v1.25.0/test/e2e/framework/test_context.go#L354-L355
+	// Turn on EmitSpecProgress to get spec progress (especially on interrupt)
+	suiteConfig.EmitSpecProgress = true
+	// Randomize specs as well as suites
+	suiteConfig.RandomizeAllSpecs = true
+	// turn off stdout/stderr capture see https://github.com/kubernetes/kubernetes/pull/111240
+	suiteConfig.OutputInterceptorMode = "none"
+	// https://github.com/kubernetes/kubernetes/blob/v1.25.0/hack/ginkgo-e2e.sh#L172-L173
+	suiteConfig.Timeout = 24 * time.Hour
+	reporterConfig.NoColor = true
+
+	ginkgo.SetReporterConfig(reporterConfig)
+	ginkgo.GetSuite().RunSpec(test.spec, ginkgo.Labels{}, "", ginkgo.GetFailer(), ginkgo.GetWriter(), suiteConfig)
+
+	var summary types.SpecReport
+	for _, report := range ginkgo.GetSuite().GetReport().SpecReports {
+		if report.NumAttempts > 0 {
+			summary = report
+		}
+	}
+
+	switch {
+	case summary.State == types.SpecStatePassed:
+		// do nothing
+	case summary.State == types.SpecStateSkipped:
+		if len(summary.Failure.Message) > 0 {
+			fmt.Fprintf(opt.ErrOut, "skip [%s:%d]: %s\n", lastFilenameSegment(summary.Failure.Location.FileName), summary.Failure.Location.LineNumber, summary.Failure.Message)
+		}
+		if len(summary.Failure.ForwardedPanic) > 0 {
+			fmt.Fprintf(opt.ErrOut, "skip [%s:%d]: %s\n", lastFilenameSegment(summary.Failure.Location.FileName), summary.Failure.Location.LineNumber, summary.Failure.ForwardedPanic)
+		}
+		return ExitError{Code: 3}
+	case summary.State == types.SpecStateFailed, summary.State == types.SpecStatePanicked, summary.State == types.SpecStateInterrupted:
+		if len(summary.Failure.ForwardedPanic) > 0 {
+			if len(summary.Failure.Location.FullStackTrace) > 0 {
+				fmt.Fprintf(opt.ErrOut, "\n%s\n", summary.Failure.Location.FullStackTrace)
+			}
+			fmt.Fprintf(opt.ErrOut, "fail [%s:%d]: Test Panicked: %s\n", lastFilenameSegment(summary.Failure.Location.FileName), summary.Failure.Location.LineNumber, summary.Failure.ForwardedPanic)
+			return ExitError{Code: 1}
+		}
+		fmt.Fprintf(opt.ErrOut, "fail [%s:%d]: %s\n", lastFilenameSegment(summary.Failure.Location.FileName), summary.Failure.Location.LineNumber, summary.Failure.Message)
+		return ExitError{Code: 1}
+	default:
+		return fmt.Errorf("unrecognized test case outcome: %#v", summary)
+	}
+	return nil
+}
+
+func (opt *TestOptions) Fail() {
+	// this function allows us to pass TestOptions as the first argument,
+	// it's empty becase we have failure check mechanism implemented above.
+}
+
+func lastFilenameSegment(filename string) string {
+	if parts := strings.Split(filename, "/vendor/"); len(parts) > 1 {
+		return parts[len(parts)-1]
+	}
+	if parts := strings.Split(filename, "/src/"); len(parts) > 1 {
+		return parts[len(parts)-1]
+	}
+	return filename
+}
+
+func testsForSuite() []*TestCase {
+	var tests []*TestCase
+
+	// Don't build the tree multiple times, it results in multiple initing of tests
+	if !ginkgo.GetSuite().InPhaseBuildTree() {
+		ginkgo.GetSuite().BuildTree()
+	}
+
+	ginkgo.GetSuite().WalkTests(func(name string, spec types.TestSpec) {
+		testCase := &TestCase{
+			Name:      spec.Text(),
+			locations: spec.CodeLocations(),
+			spec:      spec,
+		}
+		if labels, ok := generated.Annotations[name]; ok {
+			testCase.Labels = labels
+		}
+		tests = append(tests, testCase)
+	})
+	return tests
+}
diff --git a/openshift-hack/cmd/k8s-tests/types.go b/openshift-hack/cmd/k8s-tests/types.go
new file mode 100644
index 00000000000..29a0b5b5efa
--- /dev/null
+++ b/openshift-hack/cmd/k8s-tests/types.go
@@ -0,0 +1,69 @@
+package main
+
+import (
+	"fmt"
+
+	"github.com/onsi/ginkgo/v2/types"
+)
+
+// copied directly from github.com/openshift/origin/test/extended/util/cluster/cluster.go
+type ClusterConfiguration struct {
+	ProviderName string `json:"type"`
+
+	// These fields (and the "type" tag for ProviderName) chosen to match
+	// upstream's e2e.CloudConfig.
+	ProjectID   string
+	Region      string
+	Zone        string
+	NumNodes    int
+	MultiMaster bool
+	MultiZone   bool
+	Zones       []string
+	ConfigFile  string
+
+	// Disconnected is set for test jobs without external internet connectivity
+	Disconnected bool
+
+	// SingleReplicaTopology is set for disabling disruptive tests or tests
+	// that require high availability
+	SingleReplicaTopology bool
+
+	// NetworkPlugin is the "official" plugin name
+	NetworkPlugin string
+	// NetworkPluginMode is an optional sub-identifier for the NetworkPlugin.
+	// (Currently it is only used for OpenShiftSDN.)
+	NetworkPluginMode string `json:",omitempty"`
+
+	// HasIPv4 and HasIPv6 determine whether IPv4-specific, IPv6-specific,
+	// and dual-stack-specific tests are run
+	HasIPv4 bool
+	HasIPv6 bool
+
+	// HasSCTP determines whether SCTP connectivity tests can be run in the cluster
+	HasSCTP bool
+
+	// IsProxied determines whether we are accessing the cluster through an HTTP proxy
+	IsProxied bool
+
+	// IsIBMROKS determines whether the cluster is Managed IBM Cloud (ROKS)
+	IsIBMROKS bool
+
+	// IsNoOptionalCapabilities indicates the cluster has no optional capabilities enabled
+	HasNoOptionalCapabilities bool
+}
+
+// copied directly from github.com/openshift/origin/pkg/test/ginkgo/test.go
+type TestCase struct {
+	Name      string
+	Labels    string
+	spec      types.TestSpec
+	locations []types.CodeLocation
+}
+
+type ExitError struct {
+	Code int
+}
+
+func (e ExitError) Error() string {
+	return fmt.Sprintf("exit with code %d", e.Code)
+}
diff --git a/openshift-hack/e2e/annotate/annotate.go b/openshift-hack/e2e/annotate/annotate.go
index 44a00c1f8de..096ae2a00aa 100644
--- a/openshift-hack/e2e/annotate/annotate.go
+++ b/openshift-hack/e2e/annotate/annotate.go
@@ -16,7 +16,9 @@ import (
 var reHasSig = regexp.MustCompile(`\[sig-[\w-]+\]`)
 
 // Run generates tests annotations for the targeted package.
-func Run() {
+// It accepts testMaps which defines labeling rules and filter
+// function to remove elements based on test name and their labels.
+func Run(testMaps map[string][]string, filter func(name string) bool) {
 	var errors []string
 
 	if len(os.Args) != 2 && len(os.Args) != 3 {
@@ -25,7 +27,7 @@ func Run() {
 	}
 	filename := os.Args[len(os.Args)-1]
 
-	generator := newGenerator()
+	generator := newGenerator(testMaps)
 	ginkgo.GetSuite().BuildTree()
 	ginkgo.GetSuite().WalkTests(generator.generateRename)
 	if len(generator.errors) > 0 {
@@ -68,8 +70,11 @@ func Run() {
 	}
 
 	var pairs []string
-	for from, to := range generator.output {
-		pairs = append(pairs, fmt.Sprintf("%q:\n%q,", from, to))
+	for testName, labels := range generator.output {
+		if filter(fmt.Sprintf("%s%s", testName, labels)) {
+			continue
+		}
+		pairs = append(pairs, fmt.Sprintf("%q:\n%q,", testName, labels))
 	}
 	sort.Strings(pairs)
 	contents := fmt.Sprintf(`
@@ -105,12 +110,12 @@ func init() {
 	}
 }
 
-func newGenerator() *ginkgoTestRenamer {
+func newGenerator(testMaps map[string][]string) *ginkgoTestRenamer {
 	var allLabels []string
 	matches := make(map[string]*regexp.Regexp)
 	stringMatches := make(map[string][]string)
 
-	for label, items := range TestMaps {
+	for label, items := range testMaps {
 		sort.Strings(items)
 		allLabels = append(allLabels, label)
 		var remain []string
diff --git a/openshift-hack/e2e/annotate/cmd/main.go b/openshift-hack/e2e/annotate/cmd/main.go
index 54066dc105b..c1666ce9e04 100644
--- a/openshift-hack/e2e/annotate/cmd/main.go
+++ b/openshift-hack/e2e/annotate/cmd/main.go
@@ -5,5 +5,5 @@ import (
 )
 
 func main() {
-	annotate.Run()
+	annotate.Run(annotate.TestMaps, func(name string) bool { return false })
 }
diff --git a/openshift-hack/e2e/annotate/rules.go b/openshift-hack/e2e/annotate/rules.go
index 1e967029652..7879296b579 100644
--- a/openshift-hack/e2e/annotate/rules.go
+++ b/openshift-hack/e2e/annotate/rules.go
@@ -35,7 +35,6 @@ var (
 			`\[sig-scheduling\] GPUDevicePluginAcrossRecreate \[Feature:Recreate\]`,
 
 			`\[Feature:ImageQuota\]`,                    // Quota isn't turned on by default, we should do that and then reenable these tests
-			`\[Feature:Audit\]`,                         // Needs special configuration
 			`\[Feature:LocalStorageCapacityIsolation\]`, // relies on a separate daemonset?
 			`\[sig-cloud-provider-gcp\]`,                // these test require a different configuration - note that GCE tests from the sig-cluster-lifecycle were moved to the sig-cloud-provider-gcpcluster lifecycle see https://github.com/kubernetes/kubernetes/commit/0b3d50b6dccdc4bbd0b3e411c648b092477d79ac#diff-3b1910d08fb8fd8b32956b5e264f87cb
 
@@ -111,7 +110,40 @@ var (
 			`Netpol \[LinuxOnly\] NetworkPolicy between server and client using UDP should enforce policy based on Ports`,
 			`Netpol \[LinuxOnly\] NetworkPolicy between server and client using UDP should enforce policy to allow traffic only from a pod in a different namespace based on PodSelector and NamespaceSelector`,
 
+			// The new NetworkPolicy test suite is extremely resource
+			// intensive and causes itself and other concurrently-running
+			// tests to be flaky.
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1980141
+			`\[sig-network\] Netpol `,
+
 			`Topology Hints should distribute endpoints evenly`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1908645
+			`\[sig-network\] Networking Granular Checks: Services should function for service endpoints using hostNetwork`,
+			`\[sig-network\] Networking Granular Checks: Services should function for pod-Service\(hostNetwork\)`,
+
+			// https://issues.redhat.com/browse/OCPBUGS-7125
+			`\[sig-network\] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on different nodes`,
+			`\[sig-network\] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on the same nodes`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1952460
+			`\[sig-network\] Firewall rule control plane should not expose well-known ports`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1988272
+			`\[sig-network\] Networking should provide Internet connection for containers \[Feature:Networking-IPv6\]`,
+			`\[sig-network\] Networking should provider Internet connection for containers using DNS`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1957894
+			`\[sig-node\] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1952457
+			`\[sig-node\] crictl should be able to run crictl on the node`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1953478
+			`\[sig-storage\] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV`,
+
+			// https://issues.redhat.com/browse/WRKLDS-665
+			`\[sig-scheduling\] SchedulerPreemption \[Serial\] validates pod disruption condition is added to the preempted pod`,
 		},
 		// tests that need to be temporarily disabled while the rebase is in progress.
 		"[Disabled:RebaseInProgress]": {
@@ -127,6 +159,16 @@ var (
 			`\[Driver: gluster\]`,       // OpenShift 4.x does not support Gluster
 			`Volumes GlusterFS`,         // OpenShift 4.x does not support Gluster
 			`GlusterDynamicProvisioner`, // OpenShift 4.x does not support Gluster
+
+			// Skip vSphere-specific storage tests. The standard in-tree storage tests for vSphere
+			// (prefixed with `In-tree Volumes [Driver: vsphere]`) are enough for testing this plugin.
+			// https://bugzilla.redhat.com/show_bug.cgi?id=2019115
+			`\[sig-storage\].*\[Feature:vsphere\]`,
+			// Also, our CI doesn't support topology, so disable those tests
+			`\[sig-storage\] In-tree Volumes \[Driver: vsphere\] \[Testpattern: Dynamic PV \(delayed binding\)\] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies`,
+			`\[sig-storage\] In-tree Volumes \[Driver: vsphere\] \[Testpattern: Dynamic PV \(delayed binding\)\] topology should provision a volume and schedule a pod with AllowedTopologies`,
+			`\[sig-storage\] In-tree Volumes \[Driver: vsphere\] \[Testpattern: Dynamic PV \(immediate binding\)\] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies`,
+			`\[sig-storage\] In-tree Volumes \[Driver: vsphere\] \[Testpattern: Dynamic PV \(immediate binding\)\] topology should provision a volume and schedule a pod with AllowedTopologies`,
 		},
 		// tests too slow to be part of conformance
 		"[Slow]": {
@@ -160,6 +202,13 @@ var (
 
 			`\[sig-network\] IngressClass \[Feature:Ingress\] should set default value on new IngressClass`, //https://bugzilla.redhat.com/show_bug.cgi?id=1833583
 		},
+		// Tests that don't pass on disconnected, either due to requiring
+		// internet access for GitHub (e.g. many of the s2i builds), or
+		// because of pullthrough not supporting ICSP (https://bugzilla.redhat.com/show_bug.cgi?id=1918376)
+		"[Skipped:Disconnected]": {
+			// Internet access required
+			`\[sig-network\] Networking should provide Internet connection for containers`,
+		},
 		"[Skipped:azure]": {
 			"Networking should provide Internet connection for containers", // Azure does not allow ICMP traffic to internet.
 			// Azure CSI migration changed how we treat regions without zones.
@@ -230,6 +279,85 @@ var (
 			`NetworkPolicy between server and client should support a 'default-deny-all' policy`,            // uses egress feature
 			`NetworkPolicy between server and client should stop enforcing policies after they are deleted`, // uses egress feature
 		},
+
+		// These tests are skipped when openshift-tests needs to use a proxy to reach the
+		// cluster -- either because the test won't work while proxied, or because the test
+		// itself is testing a functionality using it's own proxy.
+		"[Skipped:Proxy]": {
+			// These tests setup their own proxy, which won't work when we need to access the
+			// cluster through a proxy.
+			`\[sig-cli\] Kubectl client Simple pod should support exec through an HTTP proxy`,
+			`\[sig-cli\] Kubectl client Simple pod should support exec through kubectl proxy`,
+
+			// Kube currently uses the x/net/websockets pkg, which doesn't work with proxies.
+			// See: https://github.com/kubernetes/kubernetes/pull/103595
+			`\[sig-node\] Pods should support retrieving logs from the container over websockets`,
+			`\[sig-cli\] Kubectl Port forwarding With a server listening on localhost should support forwarding over websockets`,
+			`\[sig-cli\] Kubectl Port forwarding With a server listening on 0.0.0.0 should support forwarding over websockets`,
+			`\[sig-node\] Pods should support remote command execution over websockets`,
+
+			// These tests are flacky and require internet access
+			// See https://bugzilla.redhat.com/show_bug.cgi?id=2019375
+			`\[sig-network\] DNS should resolve DNS of partial qualified names for services`,
+			`\[sig-network\] DNS should provide DNS for the cluster`,
+			// This test does not work when using in-proxy cluster, see https://bugzilla.redhat.com/show_bug.cgi?id=2084560
+			`\[sig-network\] Networking should provide Internet connection for containers`,
+		},
+
+		"[Skipped:SingleReplicaTopology]": {
+			`\[sig-apps\] Daemon set \[Serial\] should rollback without unnecessary restarts \[Conformance\]`,
+			`\[sig-node\] NoExecuteTaintManager Single Pod \[Serial\] doesn't evict pod with tolerations from tainted nodes`,
+			`\[sig-node\] NoExecuteTaintManager Single Pod \[Serial\] eventually evict pod with finite tolerations from tainted nodes`,
+			`\[sig-node\] NoExecuteTaintManager Single Pod \[Serial\] evicts pods from tainted nodes`,
+			`\[sig-node\] NoExecuteTaintManager Single Pod \[Serial\] removing taint cancels eviction \[Disruptive\] \[Conformance\]`,
+			`\[sig-node\] NoExecuteTaintManager Multiple Pods \[Serial\] evicts pods with minTolerationSeconds \[Disruptive\] \[Conformance\]`,
+			`\[sig-node\] NoExecuteTaintManager Multiple Pods \[Serial\] only evicts pods without tolerations from tainted nodes`,
+			`\[sig-cli\] Kubectl client Kubectl taint \[Serial\] should remove all the taints with the same key off a node`,
+			`\[sig-network\] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on different nodes`,
+			`\[sig-network\] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on the same nodes`,
+		},
+
+		// Tests which can't be run/don't make sense to run against a cluster with all optional capabilities disabled
+		"[Skipped:NoOptionalCapabilities]": {
+			// Requires CSISnapshot capability
+			`\[Feature:VolumeSnapshotDataSource\]`,
+			// Requires Storage capability
+			`\[Driver: aws\]`,
+			`\[Feature:StorageProvider\]`,
+		},
+
+		// tests that don't pass under openshift-sdn multitenant mode
+		"[Skipped:Network/OpenShiftSDN/Multitenant]": {
+			`\[Feature:NetworkPolicy\]`, // not compatible with multitenant mode
+		},
+		// tests that don't pass under OVN Kubernetes
+		"[Skipped:Network/OVNKubernetes]": {
+			// ovn-kubernetes does not support named ports
+			`NetworkPolicy.*named port`,
+		},
+
+		"[Skipped:ibmroks]": {
+			// Calico is allowing the request to timeout instead of returning 'REFUSED'
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1825021 - ROKS: calico SDN results in a request timeout when accessing services with no endpoints
+			`\[sig-network\] Services should be rejected when no endpoints exist`,
+
+			// Nodes in ROKS have access to secrets in the cluster to handle encryption
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1825013 - ROKS: worker nodes have access to secrets in the cluster
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting a non-existent configmap should exit with the Forbidden error, not a NotFound error`,
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting a non-existent secret should exit with the Forbidden error, not a NotFound error`,
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting a secret for a workload the node has access to should succeed`,
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting an existing configmap should exit with the Forbidden error`,
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting an existing secret should exit with the Forbidden error`,
+
+			// Access to node external address is blocked from pods within a ROKS cluster by Calico
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1825016 - e2e: NodeAuthenticator tests use both external and internal addresses for node
+			`\[sig-auth\] \[Feature:NodeAuthenticator\] The kubelet's main port 10250 should reject requests with no credentials`,
+			`\[sig-auth\] \[Feature:NodeAuthenticator\] The kubelet can delegate ServiceAccount tokens to the API server`,
+
+			// Mode returned by RHEL7 worker contains an extra character not expected by the test: dgtrwx vs dtrwx
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1825024 - e2e: Failing test - HostPath should give a volume the correct mode
+			`\[sig-storage\] HostPath should give a volume the correct mode`,
+		},
 	}
 
 	ExcludedTests = []string{
diff --git a/openshift-hack/e2e/annotate/rules_test.go b/openshift-hack/e2e/annotate/rules_test.go
index dd05c1a3ace..b929a573d90 100644
--- a/openshift-hack/e2e/annotate/rules_test.go
+++ b/openshift-hack/e2e/annotate/rules_test.go
@@ -65,7 +65,7 @@ func TestStockRules(t *testing.T) {
 
 	for _, test := range tests {
 		t.Run(test.name, func(t *testing.T) {
-			testRenamer := newGenerator()
+			testRenamer := newGenerator(TestMaps)
 			testNode := &testNode{
 				text: test.testName,
 			}
diff --git a/openshift-hack/e2e/namespace.go b/openshift-hack/e2e/namespace.go
new file mode 100644
index 00000000000..7c3ba70dfa3
--- /dev/null
+++ b/openshift-hack/e2e/namespace.go
@@ -0,0 +1,160 @@
+package e2e
+
+import (
+	"context"
+	"fmt"
+	"runtime/debug"
+	"strings"
+
+	"github.com/onsi/ginkgo/v2"
+
+	corev1 "k8s.io/api/core/v1"
+	rbacv1 "k8s.io/api/rbac/v1"
+	apierrs "k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/wait"
+	kclientset "k8s.io/client-go/kubernetes"
+	rbacv1client "k8s.io/client-go/kubernetes/typed/rbac/v1"
+	"k8s.io/client-go/util/retry"
+	"k8s.io/kubernetes/test/e2e/framework"
+
+	projectv1 "github.com/openshift/api/project/v1"
+	securityv1client "github.com/openshift/client-go/security/clientset/versioned"
+)
+
+// CreateTestingNS ensures that kubernetes e2e tests have their service accounts in the privileged and anyuid SCCs
+func CreateTestingNS(baseName string, c kclientset.Interface, labels map[string]string, isKubeNamespace bool) (*corev1.Namespace, error) {
+	if !strings.HasPrefix(baseName, "e2e-") {
+		baseName = "e2e-" + baseName
+	}
+
+	if isKubeNamespace {
+		if labels == nil {
+			labels = map[string]string{}
+		}
+		labels["security.openshift.io/disable-securitycontextconstraints"] = "true"
+	}
+
+	ns, err := framework.CreateTestingNS(context.Background(), baseName, c, labels)
+	if err != nil {
+		return ns, err
+	}
+
+	if !isKubeNamespace {
+		return ns, err
+	}
+
+	// Add anyuid and privileged permissions for upstream tests
+	clientConfig, err := framework.LoadConfig()
+	if err != nil {
+		return ns, err
+	}
+
+	securityClient, err := securityv1client.NewForConfig(clientConfig)
+	if err != nil {
+		return ns, err
+	}
+	framework.Logf("About to run a Kube e2e test, ensuring namespace/%s is privileged", ns.Name)
+	// add the "privileged" scc to ensure pods that explicitly
+	// request extra capabilities are not rejected
+	addE2EServiceAccountsToSCC(securityClient, []corev1.Namespace{*ns}, "privileged")
+	// add the "anyuid" scc to ensure pods that don't specify a
+	// uid don't get forced into a range (mimics upstream
+	// behavior)
+	addE2EServiceAccountsToSCC(securityClient, []corev1.Namespace{*ns}, "anyuid")
+	// add the "hostmount-anyuid" scc to ensure pods using hostPath
+	// can execute tests
+	addE2EServiceAccountsToSCC(securityClient, []corev1.Namespace{*ns}, "hostmount-anyuid")
+
+	// The intra-pod test requires that the service account have
+	// permission to retrieve service endpoints.
+	rbacClient, err := rbacv1client.NewForConfig(clientConfig)
+	if err != nil {
+		return ns, err
+	}
+	addRoleToE2EServiceAccounts(rbacClient, []corev1.Namespace{*ns}, "view")
+
+	// in practice too many kube tests ignore scheduling constraints
+	allowAllNodeScheduling(c, ns.Name)
+
+	return ns, err
+}
+
+var longRetry = wait.Backoff{Steps: 100}
+
+// TODO: ideally this should be rewritten to use dynamic client, not to rely on openshift types
+func addE2EServiceAccountsToSCC(securityClient securityv1client.Interface, namespaces []corev1.Namespace, sccName string) {
+	// Because updates can race, we need to set the backoff retries to be > than the number of possible
+	// parallel jobs starting at once. Set very high to allow future high parallelism.
+	err := retry.RetryOnConflict(longRetry, func() error {
+		scc, err := securityClient.SecurityV1().SecurityContextConstraints().Get(context.Background(), sccName, metav1.GetOptions{})
+		if err != nil {
+			if apierrs.IsNotFound(err) {
+				return nil
+			}
+			return err
+		}
+
+		for _, ns := range namespaces {
+			scc.Groups = append(scc.Groups, fmt.Sprintf("system:serviceaccounts:%s", ns.Name))
+		}
+		if _, err := securityClient.SecurityV1().SecurityContextConstraints().Update(context.Background(), scc, metav1.UpdateOptions{}); err != nil {
+			return err
+		}
+		return nil
+	})
+	if err != nil {
+		fatalErr(err)
+	}
+}
+
+func fatalErr(msg interface{}) {
+	// the path that leads to this being called isn't always clear...
+	fmt.Fprintln(ginkgo.GinkgoWriter, string(debug.Stack()))
+	framework.Failf("%v", msg)
+}
+
+func addRoleToE2EServiceAccounts(rbacClient rbacv1client.RbacV1Interface, namespaces []corev1.Namespace, roleName string) {
+	err := retry.RetryOnConflict(longRetry, func() error {
+		for _, ns := range namespaces {
+			if ns.Status.Phase != corev1.NamespaceTerminating {
+				_, err := rbacClient.RoleBindings(ns.Name).Create(context.Background(), &rbacv1.RoleBinding{
+					ObjectMeta: metav1.ObjectMeta{GenerateName: "default-" + roleName, Namespace: ns.Name},
+					RoleRef: rbacv1.RoleRef{
+						Kind: "ClusterRole",
+						Name: roleName,
+					},
+					Subjects: []rbacv1.Subject{
+						{Name: "default", Namespace: ns.Name, Kind: rbacv1.ServiceAccountKind},
+					},
+				}, metav1.CreateOptions{})
+				if err != nil {
+					framework.Logf("Warning: Failed to add role to e2e service account: %v", err)
+				}
+			}
+		}
+		return nil
+	})
+	if err != nil {
+		fatalErr(err)
+	}
+}
+
+// allowAllNodeScheduling sets the annotation on namespace that allows all nodes to be scheduled onto.
+func allowAllNodeScheduling(c kclientset.Interface, namespace string) {
+	err := retry.RetryOnConflict(longRetry, func() error {
+		ns, err := c.CoreV1().Namespaces().Get(context.Background(), namespace, metav1.GetOptions{})
+		if err != nil {
+			return err
+		}
+		if ns.Annotations == nil {
+			ns.Annotations = make(map[string]string)
+		}
+		ns.Annotations[projectv1.ProjectNodeSelector] = ""
+		_, err = c.CoreV1().Namespaces().Update(context.Background(), ns, metav1.UpdateOptions{})
+		return err
+	})
+	if err != nil {
+		fatalErr(err)
+	}
+}
diff --git a/openshift-hack/images/hyperkube/Dockerfile.rhel b/openshift-hack/images/hyperkube/Dockerfile.rhel
index 89a30c4c2f9..c763f726eda 100644
--- a/openshift-hack/images/hyperkube/Dockerfile.rhel
+++ b/openshift-hack/images/hyperkube/Dockerfile.rhel
@@ -1,10 +1,10 @@
 FROM registry.ci.openshift.org/ocp/builder:rhel-8-golang-1.20-openshift-4.14 AS builder
 WORKDIR /go/src/k8s.io/kubernetes
 COPY . .
-RUN make WHAT='cmd/kube-apiserver cmd/kube-controller-manager cmd/kube-scheduler cmd/kubelet cmd/watch-termination' && \
+RUN make WHAT='cmd/kube-apiserver cmd/kube-controller-manager cmd/kube-scheduler cmd/kubelet cmd/watch-termination openshift-hack/cmd/k8s-tests' && \
     mkdir -p /tmp/build && \
     cp openshift-hack/images/hyperkube/hyperkube openshift-hack/images/hyperkube/kubensenter /tmp/build && \
-    cp /go/src/k8s.io/kubernetes/_output/local/bin/linux/$(go env GOARCH)/{kube-apiserver,kube-controller-manager,kube-scheduler,kubelet,watch-termination} \
+    cp /go/src/k8s.io/kubernetes/_output/local/bin/linux/$(go env GOARCH)/{kube-apiserver,kube-controller-manager,kube-scheduler,kubelet,watch-termination,k8s-tests} \
     /tmp/build
 
 FROM registry.ci.openshift.org/ocp/4.14:base
-- 
2.40.1


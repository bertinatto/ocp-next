From 5368c4f4014bddffcdb50ff9929214d6d86b32a6 Mon Sep 17 00:00:00 2001
From: Peter Hunt <pehunt@redhat.com>
Date: Thu, 17 Oct 2024 13:15:50 -0400
Subject: [PATCH] UPSTREAM: <carry>: admission: validate minimumKubeletVersion

Signed-off-by: Peter Hunt <pehunt@redhat.com>

UPSTREAM: <carry>: authorization: add minimumkubeletversion package

MinimumKubeletVersion is a way for an admin to declare that nodes any older than the
minimum version cannot authorize with the apiserver. This effectively prevents them from joining.

Doing so means the apiservers can trust newer features are usable on clusters with version skews

Signed-off-by: Peter Hunt <pehunt@redhat.com>

UPSTREAM: <carry>: authorizer: move mininum kubelet version authorizer to pkg/kubeapiserver and add authorization mode

this does require a line of code be moved from the enablement package to stop a cyclical import

Signed-off-by: Peter Hunt <pehunt@redhat.com>

UPSTREAM: <carry>: crdvalidation: move latency profile file to be agnostic of field

Signed-off-by: Peter Hunt <pehunt@redhat.com>

UPSTREAM: <carry>: features: add MinimumKubeletVersion feature

Signed-off-by: Peter Hunt <pehunt@redhat.com>
---
 ...restrict_extreme_worker_latency_profile.go | 124 --------
 ...ict_extreme_worker_latency_profile_test.go |  68 -----
 .../node/validate_node_config.go              | 218 ++++++++++++++
 .../node/validate_node_config_test.go         | 285 ++++++++++++++++++
 .../minimum_kubelet_version.go                |  90 ++++++
 .../minimum_kubelet_version_test.go           | 193 ++++++++++++
 .../enablement/intialization.go               |   5 +
 pkg/features/openshift_features.go            |   4 +
 pkg/kubeapiserver/authorizer/modes/patch.go   |   3 +-
 pkg/kubeapiserver/authorizer/patch.go         |  46 +++
 pkg/kubeapiserver/authorizer/reload.go        |  11 +
 11 files changed, 854 insertions(+), 193 deletions(-)
 delete mode 100644 openshift-kube-apiserver/admission/customresourcevalidation/node/restrict_extreme_worker_latency_profile.go
 delete mode 100644 openshift-kube-apiserver/admission/customresourcevalidation/node/restrict_extreme_worker_latency_profile_test.go
 create mode 100644 openshift-kube-apiserver/admission/customresourcevalidation/node/validate_node_config.go
 create mode 100644 openshift-kube-apiserver/admission/customresourcevalidation/node/validate_node_config_test.go
 create mode 100644 openshift-kube-apiserver/authorization/minimumkubeletversion/minimum_kubelet_version.go
 create mode 100644 openshift-kube-apiserver/authorization/minimumkubeletversion/minimum_kubelet_version_test.go

diff --git a/openshift-kube-apiserver/admission/customresourcevalidation/node/restrict_extreme_worker_latency_profile.go b/openshift-kube-apiserver/admission/customresourcevalidation/node/restrict_extreme_worker_latency_profile.go
deleted file mode 100644
index b4b63914f8d..00000000000
--- a/openshift-kube-apiserver/admission/customresourcevalidation/node/restrict_extreme_worker_latency_profile.go
+++ /dev/null
@@ -1,124 +0,0 @@
-package node
-
-import (
-	"context"
-	"fmt"
-	"io"
-
-	"k8s.io/apimachinery/pkg/api/validation"
-	"k8s.io/apimachinery/pkg/runtime"
-	"k8s.io/apimachinery/pkg/runtime/schema"
-	"k8s.io/apimachinery/pkg/util/validation/field"
-	"k8s.io/apiserver/pkg/admission"
-
-	configv1 "github.com/openshift/api/config/v1"
-	"k8s.io/kubernetes/openshift-kube-apiserver/admission/customresourcevalidation"
-)
-
-var rejectionScenarios = []struct {
-	fromProfile configv1.WorkerLatencyProfileType
-	toProfile   configv1.WorkerLatencyProfileType
-}{
-	{fromProfile: "", toProfile: configv1.LowUpdateSlowReaction},
-	{fromProfile: configv1.LowUpdateSlowReaction, toProfile: ""},
-	{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: configv1.LowUpdateSlowReaction},
-	{fromProfile: configv1.LowUpdateSlowReaction, toProfile: configv1.DefaultUpdateDefaultReaction},
-}
-
-const PluginName = "config.openshift.io/RestrictExtremeWorkerLatencyProfile"
-
-// Register registers a plugin
-func Register(plugins *admission.Plugins) {
-	plugins.Register(PluginName, func(config io.Reader) (admission.Interface, error) {
-		return customresourcevalidation.NewValidator(
-			map[schema.GroupResource]bool{
-				configv1.Resource("nodes"): true,
-			},
-			map[schema.GroupVersionKind]customresourcevalidation.ObjectValidator{
-				configv1.GroupVersion.WithKind("Node"): configNodeV1{},
-			})
-	})
-}
-
-func toConfigNodeV1(uncastObj runtime.Object) (*configv1.Node, field.ErrorList) {
-	if uncastObj == nil {
-		return nil, nil
-	}
-
-	allErrs := field.ErrorList{}
-
-	obj, ok := uncastObj.(*configv1.Node)
-	if !ok {
-		return nil, append(allErrs,
-			field.NotSupported(field.NewPath("kind"), fmt.Sprintf("%T", uncastObj), []string{"Node"}),
-			field.NotSupported(field.NewPath("apiVersion"), fmt.Sprintf("%T", uncastObj), []string{"config.openshift.io/v1"}))
-	}
-
-	return obj, nil
-}
-
-type configNodeV1 struct{}
-
-func validateConfigNodeForExtremeLatencyProfile(obj, oldObj *configv1.Node) *field.Error {
-	fromProfile := oldObj.Spec.WorkerLatencyProfile
-	toProfile := obj.Spec.WorkerLatencyProfile
-
-	for _, rejectionScenario := range rejectionScenarios {
-		if fromProfile == rejectionScenario.fromProfile && toProfile == rejectionScenario.toProfile {
-			return field.Invalid(field.NewPath("spec", "workerLatencyProfile"), obj.Spec.WorkerLatencyProfile,
-				fmt.Sprintf(
-					"cannot update worker latency profile from %q to %q as extreme profile transition is unsupported, please select any other profile with supported transition such as %q",
-					oldObj.Spec.WorkerLatencyProfile,
-					obj.Spec.WorkerLatencyProfile,
-					configv1.MediumUpdateAverageReaction,
-				),
-			)
-		}
-	}
-	return nil
-}
-
-func (configNodeV1) ValidateCreate(_ context.Context, uncastObj runtime.Object) field.ErrorList {
-	obj, allErrs := toConfigNodeV1(uncastObj)
-	if len(allErrs) > 0 {
-		return allErrs
-	}
-
-	allErrs = append(allErrs, validation.ValidateObjectMeta(&obj.ObjectMeta, false, customresourcevalidation.RequireNameCluster, field.NewPath("metadata"))...)
-
-	return allErrs
-}
-
-func (configNodeV1) ValidateUpdate(_ context.Context, uncastObj runtime.Object, uncastOldObj runtime.Object) field.ErrorList {
-	obj, allErrs := toConfigNodeV1(uncastObj)
-	if len(allErrs) > 0 {
-		return allErrs
-	}
-	oldObj, allErrs := toConfigNodeV1(uncastOldObj)
-	if len(allErrs) > 0 {
-		return allErrs
-	}
-
-	allErrs = append(allErrs, validation.ValidateObjectMetaUpdate(&obj.ObjectMeta, &oldObj.ObjectMeta, field.NewPath("metadata"))...)
-	if err := validateConfigNodeForExtremeLatencyProfile(obj, oldObj); err != nil {
-		allErrs = append(allErrs, err)
-	}
-
-	return allErrs
-}
-
-func (configNodeV1) ValidateStatusUpdate(_ context.Context, uncastObj runtime.Object, uncastOldObj runtime.Object) field.ErrorList {
-	obj, errs := toConfigNodeV1(uncastObj)
-	if len(errs) > 0 {
-		return errs
-	}
-	oldObj, errs := toConfigNodeV1(uncastOldObj)
-	if len(errs) > 0 {
-		return errs
-	}
-
-	// TODO validate the obj.  remember that status validation should *never* fail on spec validation errors.
-	errs = append(errs, validation.ValidateObjectMetaUpdate(&obj.ObjectMeta, &oldObj.ObjectMeta, field.NewPath("metadata"))...)
-
-	return errs
-}
diff --git a/openshift-kube-apiserver/admission/customresourcevalidation/node/restrict_extreme_worker_latency_profile_test.go b/openshift-kube-apiserver/admission/customresourcevalidation/node/restrict_extreme_worker_latency_profile_test.go
deleted file mode 100644
index b22c6a2da90..00000000000
--- a/openshift-kube-apiserver/admission/customresourcevalidation/node/restrict_extreme_worker_latency_profile_test.go
+++ /dev/null
@@ -1,68 +0,0 @@
-package node
-
-import (
-	"fmt"
-	"testing"
-
-	"github.com/stretchr/testify/assert"
-
-	configv1 "github.com/openshift/api/config/v1"
-)
-
-func TestValidateConfigNodeForExtremeLatencyProfile(t *testing.T) {
-	testCases := []struct {
-		fromProfile  configv1.WorkerLatencyProfileType
-		toProfile    configv1.WorkerLatencyProfileType
-		shouldReject bool
-	}{
-		// no rejections
-		{fromProfile: "", toProfile: "", shouldReject: false},
-		{fromProfile: "", toProfile: configv1.DefaultUpdateDefaultReaction, shouldReject: false},
-		{fromProfile: "", toProfile: configv1.MediumUpdateAverageReaction, shouldReject: false},
-		{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: "", shouldReject: false},
-		{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: configv1.DefaultUpdateDefaultReaction, shouldReject: false},
-		{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: configv1.MediumUpdateAverageReaction, shouldReject: false},
-		{fromProfile: configv1.MediumUpdateAverageReaction, toProfile: "", shouldReject: false},
-		{fromProfile: configv1.MediumUpdateAverageReaction, toProfile: configv1.DefaultUpdateDefaultReaction, shouldReject: false},
-		{fromProfile: configv1.MediumUpdateAverageReaction, toProfile: configv1.MediumUpdateAverageReaction, shouldReject: false},
-		{fromProfile: configv1.MediumUpdateAverageReaction, toProfile: configv1.LowUpdateSlowReaction, shouldReject: false},
-		{fromProfile: configv1.LowUpdateSlowReaction, toProfile: configv1.MediumUpdateAverageReaction, shouldReject: false},
-		{fromProfile: configv1.LowUpdateSlowReaction, toProfile: configv1.LowUpdateSlowReaction, shouldReject: false},
-
-		// rejections
-		{fromProfile: "", toProfile: configv1.LowUpdateSlowReaction, shouldReject: true},
-		{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: configv1.LowUpdateSlowReaction, shouldReject: true},
-		{fromProfile: configv1.LowUpdateSlowReaction, toProfile: "", shouldReject: true},
-		{fromProfile: configv1.LowUpdateSlowReaction, toProfile: configv1.DefaultUpdateDefaultReaction, shouldReject: true},
-	}
-
-	for _, testCase := range testCases {
-		shouldStr := "should not be"
-		if testCase.shouldReject {
-			shouldStr = "should be"
-		}
-		testCaseName := fmt.Sprintf("update from profile %s to %s %s rejected", testCase.fromProfile, testCase.toProfile, shouldStr)
-		t.Run(testCaseName, func(t *testing.T) {
-			// config node objects
-			oldObject := configv1.Node{
-				Spec: configv1.NodeSpec{
-					WorkerLatencyProfile: testCase.fromProfile,
-				},
-			}
-			newObject := configv1.Node{
-				Spec: configv1.NodeSpec{
-					WorkerLatencyProfile: testCase.toProfile,
-				},
-			}
-
-			fieldErr := validateConfigNodeForExtremeLatencyProfile(&oldObject, &newObject)
-			assert.Equal(t, testCase.shouldReject, fieldErr != nil, "latency profile from %q to %q %s rejected", testCase.fromProfile, testCase.toProfile, shouldStr)
-
-			if testCase.shouldReject {
-				assert.Equal(t, "spec.workerLatencyProfile", fieldErr.Field, "field name during for latency profile should be spec.workerLatencyProfile")
-				assert.Contains(t, fieldErr.Detail, testCase.fromProfile, "error message should contain %q", testCase.fromProfile)
-				assert.Contains(t, fieldErr.Detail, testCase.toProfile, "error message should contain %q", testCase.toProfile)
-			}
-		})
-	}
-}
diff --git a/openshift-kube-apiserver/admission/customresourcevalidation/node/validate_node_config.go b/openshift-kube-apiserver/admission/customresourcevalidation/node/validate_node_config.go
new file mode 100644
index 00000000000..355317a362f
--- /dev/null
+++ b/openshift-kube-apiserver/admission/customresourcevalidation/node/validate_node_config.go
@@ -0,0 +1,218 @@
+package node
+
+import (
+	"context"
+	"errors"
+	"fmt"
+	"io"
+
+	configv1 "github.com/openshift/api/config/v1"
+	nodelib "github.com/openshift/library-go/pkg/apiserver/node"
+
+	openshiftfeatures "github.com/openshift/api/features"
+	"k8s.io/apimachinery/pkg/api/validation"
+	"k8s.io/apimachinery/pkg/labels"
+	"k8s.io/apimachinery/pkg/runtime"
+	"k8s.io/apimachinery/pkg/runtime/schema"
+	"k8s.io/apimachinery/pkg/util/validation/field"
+	"k8s.io/apiserver/pkg/admission"
+	"k8s.io/apiserver/pkg/admission/initializer"
+	"k8s.io/apiserver/pkg/util/feature"
+	"k8s.io/client-go/informers"
+	corev1listers "k8s.io/client-go/listers/core/v1"
+	"k8s.io/component-base/featuregate"
+	"k8s.io/kubernetes/openshift-kube-apiserver/admission/customresourcevalidation"
+)
+
+var rejectionScenarios = []struct {
+	fromProfile configv1.WorkerLatencyProfileType
+	toProfile   configv1.WorkerLatencyProfileType
+}{
+	{fromProfile: "", toProfile: configv1.LowUpdateSlowReaction},
+	{fromProfile: configv1.LowUpdateSlowReaction, toProfile: ""},
+	{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: configv1.LowUpdateSlowReaction},
+	{fromProfile: configv1.LowUpdateSlowReaction, toProfile: configv1.DefaultUpdateDefaultReaction},
+}
+
+const PluginName = "config.openshift.io/ValidateConfigNodeV1"
+
+// Register registers a plugin
+func Register(plugins *admission.Plugins) {
+	plugins.Register(PluginName, func(config io.Reader) (admission.Interface, error) {
+		ret := &configNodeV1Wrapper{}
+		delegate, err := customresourcevalidation.NewValidator(
+			map[schema.GroupResource]bool{
+				configv1.Resource("nodes"): true,
+			},
+			map[schema.GroupVersionKind]customresourcevalidation.ObjectValidator{
+				configv1.GroupVersion.WithKind("Node"): &configNodeV1{
+					nodeListerFn:                 ret.getNodeLister,
+					waitForNodeInformerSyncedFn:  ret.waitForNodeInformerSyncedFn,
+					minimumKubeletVersionEnabled: feature.DefaultFeatureGate.Enabled(featuregate.Feature(openshiftfeatures.FeatureGateMinimumKubeletVersion)),
+				},
+			})
+		if err != nil {
+			return nil, err
+		}
+		ret.delegate = delegate
+		return ret, nil
+	})
+}
+
+func toConfigNodeV1(uncastObj runtime.Object) (*configv1.Node, field.ErrorList) {
+	if uncastObj == nil {
+		return nil, nil
+	}
+
+	allErrs := field.ErrorList{}
+
+	obj, ok := uncastObj.(*configv1.Node)
+	if !ok {
+		return nil, append(allErrs,
+			field.NotSupported(field.NewPath("kind"), fmt.Sprintf("%T", uncastObj), []string{"Node"}),
+			field.NotSupported(field.NewPath("apiVersion"), fmt.Sprintf("%T", uncastObj), []string{"config.openshift.io/v1"}))
+	}
+
+	return obj, nil
+}
+
+type configNodeV1 struct {
+	nodeListerFn                 func() corev1listers.NodeLister
+	waitForNodeInformerSyncedFn  func() bool
+	minimumKubeletVersionEnabled bool
+}
+
+func validateConfigNodeForExtremeLatencyProfile(obj, oldObj *configv1.Node) *field.Error {
+	fromProfile := oldObj.Spec.WorkerLatencyProfile
+	toProfile := obj.Spec.WorkerLatencyProfile
+
+	for _, rejectionScenario := range rejectionScenarios {
+		if fromProfile == rejectionScenario.fromProfile && toProfile == rejectionScenario.toProfile {
+			return field.Invalid(field.NewPath("spec", "workerLatencyProfile"), obj.Spec.WorkerLatencyProfile,
+				fmt.Sprintf(
+					"cannot update worker latency profile from %q to %q as extreme profile transition is unsupported, please select any other profile with supported transition such as %q",
+					oldObj.Spec.WorkerLatencyProfile,
+					obj.Spec.WorkerLatencyProfile,
+					configv1.MediumUpdateAverageReaction,
+				),
+			)
+		}
+	}
+	return nil
+}
+
+func (c *configNodeV1) ValidateCreate(_ context.Context, uncastObj runtime.Object) field.ErrorList {
+	obj, allErrs := toConfigNodeV1(uncastObj)
+	if len(allErrs) > 0 {
+		return allErrs
+	}
+
+	allErrs = append(allErrs, validation.ValidateObjectMeta(&obj.ObjectMeta, false, customresourcevalidation.RequireNameCluster, field.NewPath("metadata"))...)
+	if err := c.validateMinimumKubeletVersion(obj); err != nil {
+		allErrs = append(allErrs, err)
+	}
+
+	return allErrs
+}
+
+func (c *configNodeV1) ValidateUpdate(_ context.Context, uncastObj runtime.Object, uncastOldObj runtime.Object) field.ErrorList {
+	obj, allErrs := toConfigNodeV1(uncastObj)
+	if len(allErrs) > 0 {
+		return allErrs
+	}
+	oldObj, allErrs := toConfigNodeV1(uncastOldObj)
+	if len(allErrs) > 0 {
+		return allErrs
+	}
+
+	allErrs = append(allErrs, validation.ValidateObjectMetaUpdate(&obj.ObjectMeta, &oldObj.ObjectMeta, field.NewPath("metadata"))...)
+	if err := validateConfigNodeForExtremeLatencyProfile(obj, oldObj); err != nil {
+		allErrs = append(allErrs, err)
+	}
+	if err := c.validateMinimumKubeletVersion(obj); err != nil {
+		allErrs = append(allErrs, err)
+	}
+
+	return allErrs
+}
+func (c *configNodeV1) validateMinimumKubeletVersion(obj *configv1.Node) *field.Error {
+	if !c.minimumKubeletVersionEnabled {
+		return nil
+	}
+	fieldPath := field.NewPath("spec", "minimumKubeletVersion")
+	if !c.waitForNodeInformerSyncedFn() {
+		return field.InternalError(fieldPath, fmt.Errorf("caches not synchronized, cannot validate minimumKubeletVersion"))
+	}
+
+	nodes, err := c.nodeListerFn().List(labels.Everything())
+	if err != nil {
+		return field.NotFound(fieldPath, fmt.Sprintf("Getting nodes to compare minimum version %v", err.Error()))
+	}
+
+	if err := nodelib.ValidateMinimumKubeletVersion(nodes, obj.Spec.MinimumKubeletVersion); err != nil {
+		if errors.Is(err, nodelib.ErrKubeletOutdated) {
+			return field.Forbidden(fieldPath, err.Error())
+		}
+		return field.Invalid(fieldPath, obj.Spec.MinimumKubeletVersion, err.Error())
+	}
+	return nil
+}
+
+func (*configNodeV1) ValidateStatusUpdate(_ context.Context, uncastObj runtime.Object, uncastOldObj runtime.Object) field.ErrorList {
+	obj, errs := toConfigNodeV1(uncastObj)
+	if len(errs) > 0 {
+		return errs
+	}
+	oldObj, errs := toConfigNodeV1(uncastOldObj)
+	if len(errs) > 0 {
+		return errs
+	}
+
+	// TODO validate the obj.  remember that status validation should *never* fail on spec validation errors.
+	errs = append(errs, validation.ValidateObjectMetaUpdate(&obj.ObjectMeta, &oldObj.ObjectMeta, field.NewPath("metadata"))...)
+
+	return errs
+}
+
+type configNodeV1Wrapper struct {
+	// handler is only used to know if the plugin is ready to process requests.
+	handler admission.Handler
+
+	nodeLister corev1listers.NodeLister
+	delegate   admission.ValidationInterface
+}
+
+var (
+	_ = initializer.WantsExternalKubeInformerFactory(&configNodeV1Wrapper{})
+	_ = admission.ValidationInterface(&configNodeV1Wrapper{})
+)
+
+func (c *configNodeV1Wrapper) SetExternalKubeInformerFactory(kubeInformers informers.SharedInformerFactory) {
+	nodeInformer := kubeInformers.Core().V1().Nodes()
+	c.nodeLister = nodeInformer.Lister()
+	c.handler.SetReadyFunc(nodeInformer.Informer().HasSynced)
+}
+
+func (c *configNodeV1Wrapper) ValidateInitialization() error {
+	if c.nodeLister == nil {
+		return fmt.Errorf("%s needs a nodes lister", PluginName)
+	}
+
+	return nil
+}
+
+func (c *configNodeV1Wrapper) getNodeLister() corev1listers.NodeLister {
+	return c.nodeLister
+}
+
+func (c *configNodeV1Wrapper) waitForNodeInformerSyncedFn() bool {
+	return c.handler.WaitForReady()
+}
+
+func (c *configNodeV1Wrapper) Validate(ctx context.Context, a admission.Attributes, o admission.ObjectInterfaces) (err error) {
+	return c.delegate.Validate(ctx, a, o)
+}
+
+func (c *configNodeV1Wrapper) Handles(operation admission.Operation) bool {
+	return c.delegate.Handles(operation)
+}
diff --git a/openshift-kube-apiserver/admission/customresourcevalidation/node/validate_node_config_test.go b/openshift-kube-apiserver/admission/customresourcevalidation/node/validate_node_config_test.go
new file mode 100644
index 00000000000..ddca45bd6b0
--- /dev/null
+++ b/openshift-kube-apiserver/admission/customresourcevalidation/node/validate_node_config_test.go
@@ -0,0 +1,285 @@
+package node
+
+import (
+	"fmt"
+	"testing"
+
+	"github.com/stretchr/testify/assert"
+
+	configv1 "github.com/openshift/api/config/v1"
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/validation/field"
+	corev1listers "k8s.io/client-go/listers/core/v1"
+	"k8s.io/client-go/tools/cache"
+)
+
+func TestValidateConfigNodeForExtremeLatencyProfile(t *testing.T) {
+	testCases := []struct {
+		fromProfile  configv1.WorkerLatencyProfileType
+		toProfile    configv1.WorkerLatencyProfileType
+		shouldReject bool
+	}{
+		// no rejections
+		{fromProfile: "", toProfile: "", shouldReject: false},
+		{fromProfile: "", toProfile: configv1.DefaultUpdateDefaultReaction, shouldReject: false},
+		{fromProfile: "", toProfile: configv1.MediumUpdateAverageReaction, shouldReject: false},
+		{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: "", shouldReject: false},
+		{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: configv1.DefaultUpdateDefaultReaction, shouldReject: false},
+		{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: configv1.MediumUpdateAverageReaction, shouldReject: false},
+		{fromProfile: configv1.MediumUpdateAverageReaction, toProfile: "", shouldReject: false},
+		{fromProfile: configv1.MediumUpdateAverageReaction, toProfile: configv1.DefaultUpdateDefaultReaction, shouldReject: false},
+		{fromProfile: configv1.MediumUpdateAverageReaction, toProfile: configv1.MediumUpdateAverageReaction, shouldReject: false},
+		{fromProfile: configv1.MediumUpdateAverageReaction, toProfile: configv1.LowUpdateSlowReaction, shouldReject: false},
+		{fromProfile: configv1.LowUpdateSlowReaction, toProfile: configv1.MediumUpdateAverageReaction, shouldReject: false},
+		{fromProfile: configv1.LowUpdateSlowReaction, toProfile: configv1.LowUpdateSlowReaction, shouldReject: false},
+
+		// rejections
+		{fromProfile: "", toProfile: configv1.LowUpdateSlowReaction, shouldReject: true},
+		{fromProfile: configv1.DefaultUpdateDefaultReaction, toProfile: configv1.LowUpdateSlowReaction, shouldReject: true},
+		{fromProfile: configv1.LowUpdateSlowReaction, toProfile: "", shouldReject: true},
+		{fromProfile: configv1.LowUpdateSlowReaction, toProfile: configv1.DefaultUpdateDefaultReaction, shouldReject: true},
+	}
+
+	for _, testCase := range testCases {
+		shouldStr := "should not be"
+		if testCase.shouldReject {
+			shouldStr = "should be"
+		}
+		testCaseName := fmt.Sprintf("update from profile %s to %s %s rejected", testCase.fromProfile, testCase.toProfile, shouldStr)
+		t.Run(testCaseName, func(t *testing.T) {
+			// config node objects
+			oldObject := configv1.Node{
+				Spec: configv1.NodeSpec{
+					WorkerLatencyProfile: testCase.fromProfile,
+				},
+			}
+			newObject := configv1.Node{
+				Spec: configv1.NodeSpec{
+					WorkerLatencyProfile: testCase.toProfile,
+				},
+			}
+
+			fieldErr := validateConfigNodeForExtremeLatencyProfile(&oldObject, &newObject)
+			assert.Equal(t, testCase.shouldReject, fieldErr != nil, "latency profile from %q to %q %s rejected", testCase.fromProfile, testCase.toProfile, shouldStr)
+
+			if testCase.shouldReject {
+				assert.Equal(t, "spec.workerLatencyProfile", fieldErr.Field, "field name during for latency profile should be spec.workerLatencyProfile")
+				assert.Contains(t, fieldErr.Detail, testCase.fromProfile, "error message should contain %q", testCase.fromProfile)
+				assert.Contains(t, fieldErr.Detail, testCase.toProfile, "error message should contain %q", testCase.toProfile)
+			}
+		})
+	}
+}
+
+func TestValidateConfigNodeForMinimumKubeletVersion(t *testing.T) {
+	testCases := []struct {
+		name         string
+		version      string
+		shouldReject bool
+		nodes        []*v1.Node
+		nodeListErr  error
+		errType      field.ErrorType
+		errMsg       string
+	}{
+		// no rejections
+		{
+			name:         "should not reject when minimum kubelet version is empty",
+			version:      "",
+			shouldReject: false,
+		},
+		{
+			name:         "should reject when min kubelet version bogus",
+			version:      "bogus",
+			shouldReject: true,
+			nodes: []*v1.Node{
+				{
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "1.30.0",
+						},
+					},
+				},
+			},
+			errType: field.ErrorTypeInvalid,
+			errMsg:  "failed to parse submitted version bogus No Major.Minor.Patch elements found",
+		},
+		{
+			name:         "should reject when kubelet version is bogus",
+			version:      "1.30.0",
+			shouldReject: true,
+			nodes: []*v1.Node{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "node",
+					},
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "bogus",
+						},
+					},
+				},
+			},
+			errType: field.ErrorTypeInvalid,
+			errMsg:  "failed to parse node version bogus: No Major.Minor.Patch elements found",
+		},
+		{
+			name:         "should reject when kubelet version is too old",
+			version:      "1.30.0",
+			shouldReject: true,
+			nodes: []*v1.Node{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "node",
+					},
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "1.29.0",
+						},
+					},
+				},
+			},
+			errType: field.ErrorTypeForbidden,
+			errMsg:  "kubelet version is 1.29.0, which is lower than minimumKubeletVersion of 1.30.0",
+		},
+		{
+			name:         "should reject when one kubelet version is too old",
+			version:      "1.30.0",
+			shouldReject: true,
+			nodes: []*v1.Node{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "node",
+					},
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "1.30.0",
+						},
+					},
+				},
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "node2",
+					},
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "1.29.0",
+						},
+					},
+				},
+			},
+			errType: field.ErrorTypeForbidden,
+			errMsg:  "kubelet version is 1.29.0, which is lower than minimumKubeletVersion of 1.30.0",
+		},
+		{
+			name:         "should not reject when kubelet version is equal",
+			version:      "1.30.0",
+			shouldReject: false,
+			nodes: []*v1.Node{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "node",
+					},
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "1.30.0",
+						},
+					},
+				},
+			},
+		},
+		{
+			name:         "should reject when min version incomplete",
+			version:      "1.30",
+			shouldReject: true,
+			nodes: []*v1.Node{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "node",
+					},
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "1.30.0",
+						},
+					},
+				},
+			},
+			errType: field.ErrorTypeInvalid,
+			errMsg:  "failed to parse submitted version 1.30 No Major.Minor.Patch elements found",
+		},
+		{
+			name:         "should reject when kubelet version incomplete",
+			version:      "1.30.0",
+			shouldReject: true,
+			nodes: []*v1.Node{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "node",
+					},
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "1.30",
+						},
+					},
+				},
+			},
+			errType: field.ErrorTypeInvalid,
+			errMsg:  "failed to parse node version 1.30: No Major.Minor.Patch elements found",
+		},
+		{
+			name:         "should not reject when kubelet version is new enough",
+			version:      "1.30.0",
+			shouldReject: false,
+			nodes: []*v1.Node{
+				{
+					ObjectMeta: metav1.ObjectMeta{
+						Name: "node",
+					},
+					Status: v1.NodeStatus{
+						NodeInfo: v1.NodeSystemInfo{
+							KubeletVersion: "1.31.0",
+						},
+					},
+				},
+			},
+		},
+	}
+	for _, testCase := range testCases {
+		shouldStr := "should not be"
+		if testCase.shouldReject {
+			shouldStr = "should be"
+		}
+		t.Run(testCase.name, func(t *testing.T) {
+			obj := configv1.Node{
+				Spec: configv1.NodeSpec{
+					MinimumKubeletVersion: testCase.version,
+				},
+			}
+			v := &configNodeV1{
+				nodeListerFn: fakeNodeLister(testCase.nodes),
+				waitForNodeInformerSyncedFn: func() bool {
+					return true
+				},
+				minimumKubeletVersionEnabled: true,
+			}
+
+			fieldErr := v.validateMinimumKubeletVersion(&obj)
+			assert.Equal(t, testCase.shouldReject, fieldErr != nil, "minimum kubelet version %q %s rejected", testCase.version, shouldStr)
+			if testCase.shouldReject {
+				assert.Equal(t, "spec.minimumKubeletVersion", fieldErr.Field, "field name during for mininumKubeletVersion should be spec.mininumKubeletVersion")
+				assert.Equal(t, fieldErr.Type, testCase.errType, "error type should be %q", testCase.errType)
+				assert.Contains(t, fieldErr.Detail, testCase.errMsg, "error message should contain %q", testCase.errMsg)
+			}
+		})
+	}
+}
+
+func fakeNodeLister(nodes []*v1.Node) func() corev1listers.NodeLister {
+	indexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})
+	for _, node := range nodes {
+		_ = indexer.Add(node)
+	}
+	return func() corev1listers.NodeLister {
+		return corev1listers.NewNodeLister(indexer)
+	}
+}
diff --git a/openshift-kube-apiserver/authorization/minimumkubeletversion/minimum_kubelet_version.go b/openshift-kube-apiserver/authorization/minimumkubeletversion/minimum_kubelet_version.go
new file mode 100644
index 00000000000..f28ff0e7287
--- /dev/null
+++ b/openshift-kube-apiserver/authorization/minimumkubeletversion/minimum_kubelet_version.go
@@ -0,0 +1,90 @@
+package minimumkubeletversion
+
+import (
+	"context"
+	"errors"
+	"fmt"
+
+	"github.com/blang/semver/v4"
+	openshiftfeatures "github.com/openshift/api/features"
+	nodelib "github.com/openshift/library-go/pkg/apiserver/node"
+	authorizationv1 "k8s.io/api/authorization/v1"
+	"k8s.io/apimachinery/pkg/runtime/schema"
+	"k8s.io/apiserver/pkg/authorization/authorizer"
+	"k8s.io/apiserver/pkg/util/feature"
+	v1listers "k8s.io/client-go/listers/core/v1"
+	cache "k8s.io/client-go/tools/cache"
+	"k8s.io/component-base/featuregate"
+	api "k8s.io/kubernetes/pkg/apis/core"
+	"k8s.io/kubernetes/pkg/auth/nodeidentifier"
+)
+
+type minimumKubeletVersionAuth struct {
+	nodeIdentifier          nodeidentifier.NodeIdentifier
+	nodeLister              v1listers.NodeLister
+	minVersion              *semver.Version
+	hasNodeInformerSyncedFn func() bool // factored for unit tests
+}
+
+// Creates a new minimumKubeletVersionAuth object, which is an authorizer that checks
+// whether nodes are new enough to be authorized.
+func NewMinimumKubeletVersion(minVersion *semver.Version,
+	nodeIdentifier nodeidentifier.NodeIdentifier,
+	nodeInformer cache.SharedIndexInformer,
+	nodeLister v1listers.NodeLister,
+) *minimumKubeletVersionAuth {
+	if !feature.DefaultFeatureGate.Enabled(featuregate.Feature(openshiftfeatures.FeatureGateMinimumKubeletVersion)) {
+		minVersion = nil
+	}
+
+	return &minimumKubeletVersionAuth{
+		nodeIdentifier:          nodeIdentifier,
+		nodeLister:              nodeLister,
+		hasNodeInformerSyncedFn: nodeInformer.HasSynced,
+		minVersion:              minVersion,
+	}
+}
+
+func (m *minimumKubeletVersionAuth) Authorize(ctx context.Context, attrs authorizer.Attributes) (authorizer.Decision, string, error) {
+	if m.minVersion == nil {
+		return authorizer.DecisionNoOpinion, "", nil
+	}
+
+	// Short-circut if "subjectaccessreviews", or a "get" or "update" on the node object.
+	// Regardless of kubelet version, it should be allowed to do these things.
+	if attrs.IsResourceRequest() {
+		requestResource := schema.GroupResource{Group: attrs.GetAPIGroup(), Resource: attrs.GetResource()}
+		switch requestResource {
+		case api.Resource("nodes"):
+			if v := attrs.GetVerb(); v == "get" || v == "update" {
+				return authorizer.DecisionNoOpinion, "", nil
+			}
+		case authorizationv1.Resource("subjectaccessreviews"):
+			return authorizer.DecisionNoOpinion, "", nil
+		}
+	}
+
+	nodeName, isNode := m.nodeIdentifier.NodeIdentity(attrs.GetUser())
+	if !isNode {
+		// ignore requests from non-nodes
+		return authorizer.DecisionNoOpinion, "", nil
+	}
+
+	if !m.hasNodeInformerSyncedFn() {
+		return authorizer.DecisionDeny, "", fmt.Errorf("node informer not synced, cannot check if node %s is new enough", nodeName)
+	}
+
+	node, err := m.nodeLister.Get(nodeName)
+	if err != nil {
+		return authorizer.DecisionDeny, "", err
+	}
+
+	if err := nodelib.IsNodeTooOld(node, m.minVersion); err != nil {
+		if errors.Is(err, nodelib.ErrKubeletOutdated) {
+			return authorizer.DecisionDeny, err.Error(), nil
+		}
+		return authorizer.DecisionDeny, "", err
+	}
+
+	return authorizer.DecisionNoOpinion, "", nil
+}
diff --git a/openshift-kube-apiserver/authorization/minimumkubeletversion/minimum_kubelet_version_test.go b/openshift-kube-apiserver/authorization/minimumkubeletversion/minimum_kubelet_version_test.go
new file mode 100644
index 00000000000..a43321e67e1
--- /dev/null
+++ b/openshift-kube-apiserver/authorization/minimumkubeletversion/minimum_kubelet_version_test.go
@@ -0,0 +1,193 @@
+package minimumkubeletversion
+
+import (
+	"context"
+	"strings"
+	"testing"
+
+	"github.com/blang/semver/v4"
+	authorizationv1 "k8s.io/api/authorization/v1"
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apiserver/pkg/authentication/user"
+	kauthorizer "k8s.io/apiserver/pkg/authorization/authorizer"
+	"k8s.io/client-go/informers"
+	"k8s.io/client-go/kubernetes/fake"
+	"k8s.io/kubernetes/pkg/auth/nodeidentifier"
+	"k8s.io/kubernetes/pkg/controller"
+)
+
+func TestAuthorize(t *testing.T) {
+	nodeUser := &user.DefaultInfo{Name: "system:node:node0", Groups: []string{"system:nodes"}}
+
+	testCases := []struct {
+		name            string
+		minVersion      string
+		attributes      kauthorizer.AttributesRecord
+		expectedAllowed kauthorizer.Decision
+		expectedErr     string
+		expectedMsg     string
+		node            *v1.Node
+	}{
+		{
+			name:            "no version",
+			minVersion:      "",
+			expectedAllowed: kauthorizer.DecisionNoOpinion,
+			expectedErr:     "",
+			node:            &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "name"}},
+		},
+		{
+			name:       "user not a node",
+			minVersion: "1.30.0",
+			attributes: kauthorizer.AttributesRecord{
+				ResourceRequest: true,
+				Namespace:       "ns",
+				User:            &user.DefaultInfo{Name: "name"},
+			},
+			expectedAllowed: kauthorizer.DecisionNoOpinion,
+			node:            &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node0"}},
+		},
+		{
+			name:       "skips if subjectaccessreviews",
+			minVersion: "1.30.0",
+			attributes: kauthorizer.AttributesRecord{
+				ResourceRequest: true,
+				Namespace:       "ns",
+				User:            nodeUser,
+				Resource:        "subjectaccessreviews",
+				APIGroup:        authorizationv1.GroupName,
+			},
+			expectedAllowed: kauthorizer.DecisionNoOpinion,
+			node:            &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node0"}},
+		},
+		{
+			name:       "skips if get node",
+			minVersion: "1.30.0",
+			attributes: kauthorizer.AttributesRecord{
+				ResourceRequest: true,
+				Namespace:       "ns",
+				User:            nodeUser,
+				Resource:        "nodes",
+				Verb:            "get",
+			},
+			expectedAllowed: kauthorizer.DecisionNoOpinion,
+			node:            &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node0"}},
+		},
+		{
+			name:       "skips if update nodes",
+			minVersion: "1.30.0",
+			attributes: kauthorizer.AttributesRecord{
+				ResourceRequest: true,
+				Namespace:       "ns",
+				User:            nodeUser,
+				Resource:        "nodes",
+				Verb:            "update",
+			},
+			expectedAllowed: kauthorizer.DecisionNoOpinion,
+			node:            &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node0"}},
+		},
+		{
+			name:       "fail if update node not found",
+			minVersion: "1.30.0",
+			attributes: kauthorizer.AttributesRecord{
+				ResourceRequest: true,
+				Namespace:       "ns",
+				User:            nodeUser,
+			},
+			expectedAllowed: kauthorizer.DecisionDeny,
+			expectedErr:     `node "node0" not found`,
+			node:            &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node1"}},
+		},
+		{
+			name:       "skip if bogus kubelet version",
+			minVersion: "1.30.0",
+			attributes: kauthorizer.AttributesRecord{
+				ResourceRequest: true,
+				Namespace:       "ns",
+				User:            nodeUser,
+			},
+			expectedAllowed: kauthorizer.DecisionDeny,
+			expectedErr:     `failed to parse node version bogus: No Major.Minor.Patch elements found`,
+			node: &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node0"},
+				Status: v1.NodeStatus{
+					NodeInfo: v1.NodeSystemInfo{
+						KubeletVersion: "bogus",
+					},
+				}},
+		},
+		{
+			name:       "deny if too low version",
+			minVersion: "1.30.0",
+			attributes: kauthorizer.AttributesRecord{
+				ResourceRequest: true,
+				Namespace:       "ns",
+				User:            nodeUser,
+			},
+			expectedAllowed: kauthorizer.DecisionDeny,
+			expectedMsg:     `kubelet version is outdated: kubelet version is 1.29.8, which is lower than minimumKubeletVersion of 1.30.0`,
+			node: &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node0"},
+				Status: v1.NodeStatus{
+					NodeInfo: v1.NodeSystemInfo{
+						KubeletVersion: "v1.29.8-20+15d27f9ba1c119",
+					},
+				}},
+		},
+		{
+			name:       "accept if high enough version",
+			minVersion: "1.30.0",
+			attributes: kauthorizer.AttributesRecord{
+				ResourceRequest: true,
+				Namespace:       "ns",
+				User:            nodeUser,
+			},
+			expectedAllowed: kauthorizer.DecisionNoOpinion,
+			node: &v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node0"},
+				Status: v1.NodeStatus{
+					NodeInfo: v1.NodeSystemInfo{
+						KubeletVersion: "1.30.0",
+					},
+				}},
+		},
+	}
+
+	for _, tc := range testCases {
+		t.Run(tc.name, func(t *testing.T) {
+			fakeInformerFactory := informers.NewSharedInformerFactory(&fake.Clientset{}, controller.NoResyncPeriodFunc())
+			fakeNodeInformer := fakeInformerFactory.Core().V1().Nodes()
+			fakeNodeInformer.Informer().GetStore().Add(tc.node)
+			var minVersion *semver.Version
+			if tc.minVersion != "" {
+				v := semver.MustParse(tc.minVersion)
+				minVersion = &v
+			}
+
+			authorizer := &minimumKubeletVersionAuth{
+				nodeIdentifier: nodeidentifier.NewDefaultNodeIdentifier(),
+				nodeLister:     fakeNodeInformer.Lister(),
+				minVersion:     minVersion,
+				hasNodeInformerSyncedFn: func() bool {
+					return true
+				},
+			}
+
+			actualAllowed, actualMsg, actualErr := authorizer.Authorize(context.TODO(), tc.attributes)
+			switch {
+			case len(tc.expectedErr) == 0 && actualErr == nil:
+			case len(tc.expectedErr) == 0 && actualErr != nil:
+				t.Errorf("%s: unexpected error: %v", tc.name, actualErr)
+			case len(tc.expectedErr) != 0 && actualErr == nil:
+				t.Errorf("%s: missing error: %v", tc.name, tc.expectedErr)
+			case len(tc.expectedErr) != 0 && actualErr != nil:
+				if !strings.Contains(actualErr.Error(), tc.expectedErr) {
+					t.Errorf("expected %v, got %v", tc.expectedErr, actualErr)
+				}
+			}
+			if tc.expectedMsg != actualMsg {
+				t.Errorf("expected %v, got %v", tc.expectedMsg, actualMsg)
+			}
+			if tc.expectedAllowed != actualAllowed {
+				t.Errorf("expected %v, got %v", tc.expectedAllowed, actualAllowed)
+			}
+		})
+	}
+}
diff --git a/openshift-kube-apiserver/enablement/intialization.go b/openshift-kube-apiserver/enablement/intialization.go
index 52794bec4b6..3aff4daf24c 100644
--- a/openshift-kube-apiserver/enablement/intialization.go
+++ b/openshift-kube-apiserver/enablement/intialization.go
@@ -87,6 +87,11 @@ func ForceGlobalInitializationForOpenShift() {
 	// we need to have the authorization chain place something before system:masters
 	// SkipSystemMastersAuthorizer disable implicitly added system/master authz, and turn it into another authz mode "SystemMasters", to be added via authorization-mode
 	authorizer.SkipSystemMastersAuthorizer()
+
+	// Set the minimum kubelet version
+	// If the OpenshiftConfig wasn't configured by this point, it's a programming error,
+	// and this should panic.
+	authorizer.SetMinimumKubeletVersion(OpenshiftConfig().MinimumKubeletVersion)
 }
 
 var SCCAdmissionPlugin = sccadmission.NewConstraint()
diff --git a/pkg/features/openshift_features.go b/pkg/features/openshift_features.go
index 96c0bc7469c..2ed4e14b854 100644
--- a/pkg/features/openshift_features.go
+++ b/pkg/features/openshift_features.go
@@ -14,4 +14,8 @@ func registerOpenshiftFeatures() {
 	defaultVersionedKubernetesFeatureGates[RouteExternalCertificate] = featuregate.VersionedSpecs{
 		{Version: version.MustParse("1.29"), Default: false, PreRelease: featuregate.Alpha},
 	}
+	// Introduced in 4.19
+	defaultVersionedKubernetesFeatureGates[MinimumKubeletVersion] = featuregate.VersionedSpecs{
+		{Version: version.MustParse("1.32"), Default: false, PreRelease: featuregate.Alpha},
+	}
 }
diff --git a/pkg/kubeapiserver/authorizer/modes/patch.go b/pkg/kubeapiserver/authorizer/modes/patch.go
index bc892601ebe..830982e5b71 100644
--- a/pkg/kubeapiserver/authorizer/modes/patch.go
+++ b/pkg/kubeapiserver/authorizer/modes/patch.go
@@ -2,7 +2,8 @@ package modes
 
 var ModeScope = "Scope"
 var ModeSystemMasters = "SystemMasters"
+var ModeMinimumKubeletVersion = "MinimumKubeletVersion"
 
 func init() {
-	AuthorizationModeChoices = append(AuthorizationModeChoices, ModeScope, ModeSystemMasters)
+	AuthorizationModeChoices = append(AuthorizationModeChoices, ModeScope, ModeSystemMasters, ModeMinimumKubeletVersion)
 }
diff --git a/pkg/kubeapiserver/authorizer/patch.go b/pkg/kubeapiserver/authorizer/patch.go
index 8a095efcf98..7d44be99648 100644
--- a/pkg/kubeapiserver/authorizer/patch.go
+++ b/pkg/kubeapiserver/authorizer/patch.go
@@ -1,8 +1,54 @@
 package authorizer
 
+import (
+	"sync"
+
+	"github.com/blang/semver/v4"
+)
+
 var skipSystemMastersAuthorizer = false
 
 // SkipSystemMastersAuthorizer disable implicitly added system/master authz, and turn it into another authz mode "SystemMasters", to be added via authorization-mode
 func SkipSystemMastersAuthorizer() {
 	skipSystemMastersAuthorizer = true
 }
+
+var (
+	minimumKubeletVersion *semver.Version
+	versionLock           sync.Mutex
+	versionSet            bool
+)
+
+// GetMinimumKubeletVersion retrieves the set global minimum kubelet version in a safe way.
+// It ensures it is only retrieved once, and is set before it's retrieved.
+// The global value should only be gotten through this function.
+// It is valid for the version to be unset. It will be treated the same as explicitly setting version to "".
+// This function (and the corresponding functions/variables) are added to avoid a import cycle between the
+// ./openshift-kube-apiserver/enablement and ./pkg/kubeapiserver/authorizer packages
+func GetMinimumKubeletVersion() *semver.Version {
+	versionLock.Lock()
+	defer versionLock.Unlock()
+	if !versionSet {
+		panic("coding error: MinimumKubeletVersion not set yet")
+	}
+	return minimumKubeletVersion
+}
+
+// SetMinimumKubeletVersion sets the global minimum kubelet version in a safe way.
+// It ensures it is only set once, and the passed version is valid.
+// If will panic on any error.
+// The global value should only be set through this function.
+// Passing an empty string for version is valid, and means there is no minimum version.
+func SetMinimumKubeletVersion(version string) {
+	versionLock.Lock()
+	defer versionLock.Unlock()
+	if versionSet {
+		panic("coding error: MinimumKubeletVersion already set")
+	}
+	versionSet = true
+	if len(version) == 0 {
+		return
+	}
+	v := semver.MustParse(version)
+	minimumKubeletVersion = &v
+}
diff --git a/pkg/kubeapiserver/authorizer/reload.go b/pkg/kubeapiserver/authorizer/reload.go
index 381765d81c3..3de2fff36a8 100644
--- a/pkg/kubeapiserver/authorizer/reload.go
+++ b/pkg/kubeapiserver/authorizer/reload.go
@@ -28,6 +28,7 @@ import (
 	"time"
 
 	"k8s.io/kubernetes/openshift-kube-apiserver/authorization/browsersafe"
+	"k8s.io/kubernetes/openshift-kube-apiserver/authorization/minimumkubeletversion"
 
 	"k8s.io/apimachinery/pkg/util/sets"
 	authzconfig "k8s.io/apiserver/pkg/apis/apiserver"
@@ -43,6 +44,7 @@ import (
 	webhookmetrics "k8s.io/apiserver/plugin/pkg/authorizer/webhook/metrics"
 	"k8s.io/klog/v2"
 	"k8s.io/kubernetes/pkg/auth/authorizer/abac"
+	"k8s.io/kubernetes/pkg/auth/nodeidentifier"
 	"k8s.io/kubernetes/pkg/kubeapiserver/authorizer/modes"
 	"k8s.io/kubernetes/pkg/util/filesystem"
 	"k8s.io/kubernetes/plugin/pkg/auth/authorizer/node"
@@ -175,6 +177,15 @@ func (r *reloadableAuthorizerResolver) newForConfig(authzConfig *authzconfig.Aut
 		case authzconfig.AuthorizerType(modes.ModeSystemMasters):
 			// no browsersafeauthorizer here becase that rewrites the resources.  This authorizer matches no matter which resource matches.
 			authorizers = append(authorizers, authorizerfactory.NewPrivilegedGroups(user.SystemPrivilegedGroup))
+		case authzconfig.AuthorizerType(modes.ModeMinimumKubeletVersion):
+			// Add MinimumKubeletVerison authorizer, to block a node from being able to access most resources if it's not new enough.
+			// We must do so here instead of in pkg/apiserver because it relies on a node informer, which is not present in generic control planes.
+			authorizers = append(authorizers, minimumkubeletversion.NewMinimumKubeletVersion(
+				GetMinimumKubeletVersion(),
+				nodeidentifier.NewDefaultNodeIdentifier(),
+				r.initialConfig.VersionedInformerFactory.Core().V1().Nodes().Informer(),
+				r.initialConfig.VersionedInformerFactory.Core().V1().Nodes().Lister(),
+			))
 		default:
 			return nil, nil, fmt.Errorf("unknown authorization mode %s specified", configuredAuthorizer.Type)
 		}
-- 
2.49.0

